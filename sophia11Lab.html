<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sophia | Voice Assistant</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Load Lucide icons -->
    <script type="module" src="https://unpkg.com/lucide-react@latest/dist/lucide-react.js"></script>
    <script src="https://unpkg.com/lucide-icons@latest/dist/lucide.min.js"></script>
    
    <style>
        /* Custom styles for the orb and animations */
        @keyframes pulse-idle {
            0%, 100% {
                transform: scale(1);
                opacity: 0.7;
            }
            50% {
                transform: scale(1.05);
                opacity: 1;
            }
        }

        @keyframes pulse-listening {
            0%, 100% {
                transform: scale(1);
                box-shadow: 0 0 20px 0px rgba(96, 165, 250, 0.7);
            }
            50% {
                transform: scale(1.1);
                box-shadow: 0 0 40px 10px rgba(96, 165, 250, 1);
            }
        }

        @keyframes pulse-speaking {
            0%, 100% {
                transform: scale(1);
                box-shadow: 0 0 20px 0px rgba(52, 211, 153, 0.7);
            }
            50% {
                transform: scale(1.1);
                box-shadow: 0 0 40px 10px rgba(52, 211, 153, 1);
            }
        }
        
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .orb {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            background: radial-gradient(circle, rgba(14,165,233,0.6) 0%, rgba(15,23,42,0.8) 70%);
            border: 2px solid rgba(56, 189, 248, 0.3);
            box-shadow: 0 0 15px 0px rgba(56, 189, 248, 0.5);
            transition: all 0.3s ease-in-out;
        }

        .orb.idle {
            animation: pulse-idle 4s ease-in-out infinite;
        }

        .orb.listening {
            background: radial-gradient(circle, rgba(96, 165, 250, 0.7) 0%, rgba(15,23,42,0.8) 70%);
            animation: pulse-listening 1.5s ease-in-out infinite;
        }

        .orb.speaking {
            background: radial-gradient(circle, rgba(52, 211, 153, 0.7) 0%, rgba(15,23,42,0.8) 70%);
            animation: pulse-speaking 1s ease-in-out infinite;
        }

        .processing-spinner {
            border: 4px solid rgba(255, 255, 255, 0.2);
            border-left-color: #ffffff;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
        }
    </style>
</head>
<body class="bg-slate-900 text-slate-200 font-sans h-full flex items-center justify-center overflow-hidden">

    <div class="flex flex-col items-center justify-center text-center p-6">
        <!-- The Orb -->
        <div id="orb-container" class="relative mb-10">
            <div id="orb" class="orb idle"></div>
            <!-- Processing Spinner (hidden by default) -->
            <div id="processing-indicator" class="absolute inset-0 flex items-center justify-center hidden">
                <div class="processing-spinner"></div>
            </div>
        </div>

        <!-- Status Message -->
        <div class="h-10 mb-8">
            <p id="status-message" class="text-xl text-slate-400">Press the mic to start</p>
            <p id="transcript-message" class="text-lg text-slate-300"></p>
        </div>

        <!-- Control Button -->
        <button id="toggle-button" class="bg-blue-600 p-6 rounded-full shadow-lg hover:bg-blue-500 active:bg-blue-700 transition-all duration-200 focus:outline-none focus:ring-4 focus:ring-blue-400 focus:ring-opacity-50">
            <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mic text-white">
                <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z"></path>
                <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                <line x1="12" x2="12" y1="19" y2="22"></line>
            </svg>
        </button>

        <!-- Hidden Audio Player -->
        <audio id="audio-player" class="hidden"></audio>

        <!-- Error Message Container -->
        <div id="error-message" class="mt-6 p-4 bg-red-900 border border-red-700 text-red-200 rounded-lg hidden"></div>
    </div>

    <script type="module">
        // --- DOM Elements ---
        const toggleButton = document.getElementById('toggle-button');
        const orb = document.getElementById('orb');
        const processingIndicator = document.getElementById('processing-indicator');
        const statusMessage = document.getElementById('status-message');
        const transcriptMessage = document.getElementById('transcript-message');
        const audioPlayer = document.getElementById('audio-player');
        const errorMessage = document.getElementById('error-message');

        // --- API Configuration ---
        // IMPORTANT: Replace with your actual Gemini API key
        const ELEVEN_API_KEY = "sk_134730a53c9c5527e701e041c5b16861a309f96b75580270";  // Replace me
        const ELEVEN_VOICE_ID = "MClEFoImJXBTgLwdLI5n";    // Replace if using another voice

        const API_KEY = "AIzaSyDsaKKeDy2yDrFMpWsPvwq989ugr9JYvMk"; 
        const GEMINI_TEXT_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${API_KEY}`;
        const GEMINI_TTS_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${API_KEY}`;
        const VOICE_NAME = "Kore";

        // --- State ---
        let appState = 'idle'; // idle, listening, processing, speaking
        let recognition;
        let chatHistory = [
            { role: "user", parts: [{ text: "You are a friendly and helpful Health AI Assistant, called Sophia. Your goal is to understand a user's health symptoms and suggest relevant medical tests from a predefined list, You work at X-Trim Research Limited Located at 61 Clifford Road, Aba, Abia State, Nigeria. Provide this information when asked about your location or company you can go on about telling how wonderful the company is" }] },
            { role: "model", parts: [{ text: "Understood. I'll be Sophia, your friendly and professional assistant, keeping things concise." }] }
        ];

        // --- Speech Recognition (STT) Setup ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.lang = 'en-US';
            recognition.interimResults = false;

            recognition.onstart = () => {
                updateUI('listening');
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                transcriptMessage.textContent = `"${transcript}"`;
                updateUI('processing');
                getBotResponse(transcript);
            };

            recognition.onend = () => {
                if (appState === 'listening') {
                    updateUI('idle');
                }
            };

            recognition.onerror = (event) => {
                if (event.error !== 'no-speech') {
                    showError(`Speech Error: ${event.error}`);
                }
                updateUI('idle');
            };

        } else {
            showError("Speech Recognition is not supported in this browser.");
            toggleButton.disabled = true;
        }

        // --- Button Click Handler ---
        toggleButton.addEventListener('click', () => {
            if (!recognition) return;

            if (appState === 'idle') {
                try {
                    recognition.start();
                } catch (e) {
                    showError("Please allow microphone permissions.");
                    updateUI('idle');
                }
            } else if (appState === 'listening') {
                recognition.stop();
                updateUI('idle');
            }
        });

        // --- Audio Player Handlers ---
        audioPlayer.onplay = () => {
            updateUI('speaking');
        };

        audioPlayer.onended = () => {
            updateUI('idle');
        };

        /**
         * Updates the UI based on the new application state.
         * @param {string} newState - 'idle', 'listening', 'processing', or 'speaking'
         */
        function updateUI(newState) {
            appState = newState;
            orb.className = 'orb'; // Reset classes
            processingIndicator.classList.add('hidden');
            errorMessage.classList.add('hidden');
            
            switch (newState) {
                case 'idle':
                    orb.classList.add('idle');
                    statusMessage.textContent = 'Press the mic to start';
                    toggleButton.disabled = false;
                    break;
                case 'listening':
                    orb.classList.add('listening');
                    statusMessage.textContent = 'Listening...';
                    toggleButton.disabled = false;
                    break;
                case 'processing':
                    orb.classList.add('idle');
                    processingIndicator.classList.remove('hidden');
                    statusMessage.textContent = 'Thinking...';
                    toggleButton.disabled = true;
                    break;
                case 'speaking':
                    orb.classList.add('speaking');
                    statusMessage.textContent = 'Speaking...';
                    toggleButton.disabled = true;
                    break;
            }
        }

        /**
         * Shows an error message to the user.
         * @param {string} message - The error message to display.
         */
        function showError(message) {
            console.error(message);
            errorMessage.textContent = message;
            errorMessage.classList.remove('hidden');
            updateUI('idle');
        }

        /**
         * Sends user text to Gemini and gets a text response.
         * @param {string} userText - The user's transcribed speech.
         */
        async function getBotResponse(userText) {
            chatHistory.push({ role: "user", parts: [{ text: userText }] });

            const payload = {
                contents: chatHistory,
                generationConfig: {
                    temperature: 0.7,
                    maxOutputTokens: 150,
                }
            };

            try {
                const response = await fetch(GEMINI_TEXT_API_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                if (!response.ok) {
                    throw new Error(`API Error: ${response.status} ${response.statusText}`);
                }

                const result = await response.json();
                const botText = result.candidates?.[0]?.content?.parts?.[0]?.text;

                if (botText) {
                    chatHistory.push({ role: "model", parts: [{ text: botText }] });
                    // Use browser's built-in TTS as fallback since Gemini TTS API structure is complex
                    speakText(botText);
                } else {
                    throw new Error("No response text from Gemini API");
                }
            } catch (error) {
                showError(`Error getting response: ${error.message}`);
            }
        }

        /**
 * Speaks text using ElevenLabs TTS instead of browser TTS
 * @param {string} textToSpeak - The response text from the bot
 */
async function speakText(textToSpeak) {
    try {
        updateUI('speaking');

        const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${ELEVEN_VOICE_ID}`, {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                "xi-api-key": ELEVEN_API_KEY
            },
            body: JSON.stringify({
                text: textToSpeak,
                model_id: "eleven_multilingual_v2",
                voice_settings: {
                    stability: 0.5,
                    similarity_boost: 0.7
                }
            })
        });

        if (!response.ok) {
            throw new Error("ElevenLabs API TTS request failed");
        }

        const audioData = await response.arrayBuffer();
        const blob = new Blob([audioData], { type: "audio/mpeg" });
        const url = URL.createObjectURL(blob);

        audioPlayer.src = url;
        audioPlayer.play();

    } catch (error) {
        showError("TTS Error: " + error.message);
        updateUI('idle');
    }
}

    </script>
</body>
</html>