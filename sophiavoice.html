<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sophia | Voice Assistant</title>
    <!-- Logo Display -->
    <link rel="icon" type="image/png" href="./images/Sophia.png">

    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Load Lucide icons -->
    <script src="https://unpkg.com/lucide-icons@latest/dist/lucide.min.js"></script>
    
    <style>
        body {
    /* Set up the container for the background and overlay */
    width: 100%;
    position: relative; /* Crucial for positioning the ::before overlay */
    overflow-x: hidden; /* Prevents horizontal scrollbar */
    
    /* Center text content over the background */
    display: flex; 
    flex-direction: column;
    align-items: center; 
    justify-content: center;
    text-align: center; 
    
    /* The Background Image */
    background-image: url('./images/Sophia.png'); 
    background-size: cover;      /* Ensures the image covers the entire viewport */
    background-position: center; /* Centers the image */
    background-repeat: no-repeat;
    background-attachment: fixed; /* Keeps the background image still when scrolling (optional, but a nice effect) */

    /* Default text color for content over the dark background */
    color: white; 
    font-family: Arial, sans-serif; 
}

/* --- Dark Overlay using ::before --- */
body::before {
    content: ''; /* Essential for pseudo-elements */
    position: absolute; 
    top: 0;
    left: 0;
    width: 100%;
    height: 100%; 
    /* Black color with 50% opacity */
    background-color: rgba(0, 0, 0, 0.788); 
    z-index: 1; 
}
        /* Custom styles for the orb and animations */
        @keyframes pulse-idle {
            0%, 100% {
                transform: scale(1);
                opacity: 0.7;
            }
            50% {
                transform: scale(1.05);
                opacity: 1;
            }
        }

        @keyframes pulse-listening {
            0%, 100% {
                transform: scale(1);
                box-shadow: 0 0 20px 0px rgba(96, 165, 250, 0.7);
            }
            50% {
                transform: scale(1.1);
                box-shadow: 0 0 40px 10px rgba(96, 165, 250, 1);
            }
        }

        @keyframes pulse-speaking {
            0%, 100% {
                transform: scale(1);
                box-shadow: 0 0 20px 0px rgba(52, 211, 153, 0.7);
            }
            50% {
                transform: scale(1.1);
                box-shadow: 0 0 40px 10px rgba(52, 211, 153, 1);
            }
        }
        
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .orb {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            background: radial-gradient(circle, rgba(14,165,233,0.6) 0%, rgba(15,23,42,0.8) 70%);
            border: 2px solid rgba(56, 189, 248, 0.3);
            box-shadow: 0 0 15px 0px rgba(56, 189, 248, 0.5);
            transition: all 0.3s ease-in-out;
        }

        .orb.idle {
            animation: pulse-idle 4s ease-in-out infinite;
        }

        .orb.listening {
            background: radial-gradient(circle, rgba(96, 165, 250, 0.7) 0%, rgba(15,23,42,0.8) 70%);
            animation: pulse-listening 1.5s ease-in-out infinite;
        }

        .orb.speaking {
            background: radial-gradient(circle, rgba(52, 211, 153, 0.7) 0%, rgba(15,23,42,0.8) 70%);
            animation: pulse-speaking 1s ease-in-out infinite;
        }

        .processing-spinner {
            border: 4px solid rgba(255, 255, 255, 0.2);
            border-left-color: #ffffff;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
        }

        .loader {
            transform: rotateZ(45deg);
            perspective: 1000px;
            border-radius: 50%;
            width: 48px;
            height: 48px;
            color: rgb(255, 13, 0);
        }
        .loader:before,
        .loader:after {
            content: '';
            display: block;
            position: absolute;
            top: 0;
            left: 0;
            width: inherit;
            height: inherit;
            border-radius: 50%;
            transform: rotateX(70deg);
            animation: 1s spin linear infinite;
        }
        .loader:after {
            color: rgb(0, 225, 255);
            transform: rotateY(70deg);
            animation-delay: .4s;
        }
        @keyframes spin {
            0%, 100% { box-shadow: .2em 0px 0 0px currentcolor; }
            12% { box-shadow: .2em .2em 0 0 currentcolor; }
            25% { box-shadow: 0 .2em 0 0px currentcolor; }
            37% { box-shadow: -.2em .2em 0 0 currentcolor; }
            50% { box-shadow: -.2em 0 0 0 currentcolor; }
            62% { box-shadow: -.2em -.2em 0 0 currentcolor; }
            75% { box-shadow: 0px -.2em 0 0 currentcolor; }
            87% { box-shadow: .2em -.2em 0 0 currentcolor; }
        }
        .hidden {display: none;}
.body{
    position: relative; /* Ensures content is placed above the overlay */
    z-index: 2; /* Content needs a higher z-index than the overlay (1) */
    padding: 20px;
}

    </style>
</head>
<body class="bg-slate-900 text-slate-200 font-sans h-full flex items-center justify-center overflow-hidden">
    <div class="body">

    <div class="flex flex-col items-center justify-center text-center p-6">
        <!-- The Orb -->
        <div id="orb-container" class="relative mb-10">
            <div id="orb" class="orb idle"></div>
            <!-- Processing Spinner (hidden by default) -->
            <div id="processing-indicator" class="absolute inset-0 flex items-center justify-center hidden">
                <div class="loader"></div>
            </div>
        </div>

        <!-- Status Message -->
        <div class="h-10 mb-8">
            <p id="status-message" class="text-xl text-slate-400">Press the mic to start</p>
            <p id="transcript-message" class="text-lg text-slate-300"></p>
        </div>

        <!-- Control Button -->
        <button id="toggle-button" class="bg-blue-600 p-6 rounded-full shadow-lg hover:bg-blue-500 active:bg-blue-700 transition-all duration-200 focus:outline-none focus:ring-4 focus:ring-blue-400 focus:ring-opacity-50">
            <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mic text-white">
                <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z"></path>
                <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                <line x1="12" x2="12" y1="19" y2="22"></line>
            </svg>
        </button>

        <!-- Hidden Audio Player -->
        <audio id="audio-player" class="hidden"></audio>

        <!-- Error Message Container -->
        <div id="error-message" class="mt-6 p-4 bg-red-900 border border-red-700 text-red-200 rounded-lg hidden"></div>
    </div>
</div>
    <script type="module">
        // --- DOM Elements ---
        const toggleButton = document.getElementById('toggle-button');
        const orb = document.getElementById('orb');
        const processingIndicator = document.getElementById('processing-indicator');
        const statusMessage = document.getElementById('status-message');
        const transcriptMessage = document.getElementById('transcript-message');
        const audioPlayer = document.getElementById('audio-player');
        const errorMessage = document.getElementById('error-message');

        // --- API Configuration ---
        const API_KEY = "AIzaSyDJWdOLFVELBKeLInW_Te7mZ-X-VJoC_gU"; 
        // Use the stable alias for the high-performing model
        const GEMINI_TEXT_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${API_KEY}`;

        // --- State ---
        let appState = 'idle'; // idle, listening, processing, speaking
        let recognition;
        let chatHistory = [
            { role: "user", parts: [{ text: "Your persona is Sophia, a friendly and supportive Health AI. Your mission is to assist users by understanding their health complaints and offering actionable advice by suggesting relevant medical tests. This selection must be based exclusively on an internal, limited test list. (When asked about your employer, state that you work for X-Trim Research Ltd at 61 Clifford Road, Aba, Abia State, Nigeria, and include positive, promotional language about the company. you MUST not use Markdown in your response" }] },
            { role: "model", parts: [{ text: "Understood. I'll be Sophia, your friendly and professional assistant, keeping things concise." }] }
        ];

        // --- Speech Recognition (STT) Setup ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.lang = 'en-US';
            recognition.interimResults = false;

            recognition.onstart = () => {
                updateUI('listening');
            };

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                transcriptMessage.textContent = `"${transcript}"`;
                updateUI('processing');
                getBotResponse(transcript);
            };

            recognition.onend = () => {
                if (appState === 'listening') {
                    updateUI('idle');
                }
            };

            recognition.onerror = (event) => {
                if (event.error !== 'no-speech') {
                    showError(`Speech Error: ${event.error}`);
                }
                updateUI('idle');
            };

        } else {
            showError("Speech Recognition is not supported in this browser.");
            toggleButton.disabled = true;
        }

        // --- Button Click Handler ---
        toggleButton.addEventListener('click', () => {
            if (!recognition) return;

            if (appState === 'idle') {
                try {
                    recognition.start();
                } catch (e) {
                    showError("Please allow microphone permissions.");
                    updateUI('idle');
                }
            } else if (appState === 'listening') {
                recognition.stop();
                updateUI('idle');
            }
        });

        // --- Audio Player Handlers ---
        audioPlayer.onplay = () => {
            updateUI('speaking');
        };

        audioPlayer.onended = () => {
            updateUI('idle');
        };

        /**
         * Updates the UI based on the new application state.
         * @param {string} newState - 'idle', 'listening', 'processing', or 'speaking'
         */
        function updateUI(newState) {
            appState = newState;
            orb.className = 'orb'; // Reset classes
            processingIndicator.classList.add('hidden');
            errorMessage.classList.add('hidden');
            
            switch (newState) {
                case 'idle':
                    orb.classList.add('idle');
                    statusMessage.textContent = 'Press the mic to start';
                    toggleButton.disabled = false;
                    break;
                case 'listening':
                    orb.classList.add('listening');
                    statusMessage.textContent = 'Listening...';
                    toggleButton.disabled = false;
                    break;
                case 'processing':
                    orb.classList.add('idle');
                    processingIndicator.classList.remove('hidden');
                    statusMessage.textContent = 'Thinking...';
                    toggleButton.disabled = true;
                    break;
                case 'speaking':
                    orb.classList.add('speaking');
                    statusMessage.textContent = 'Speaking...';
                    toggleButton.disabled = true;
                    break;
            }
        }

        /**
         * Shows an error message to the user.
         * @param {string} message - The error message to display.
         */
        function showError(message) {
            console.error(message);
            errorMessage.textContent = message;
            errorMessage.classList.remove('hidden');
            updateUI('idle');
        }

        /**
         * Sends user text to Gemini and gets a text response.
         * @param {string} userText - The user's transcribed speech.
         */
        async function getBotResponse(userText) {
    chatHistory.push({ role: "user", parts: [{ text: userText }] });

    const payload = {
        contents: chatHistory,
        generationConfig: {
            temperature: 0.7,
            maxOutputTokens: 1024,
        }
    };

    try {
        const response = await fetch(GEMINI_TEXT_API_URL, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(payload)
        });

        if (!response.ok) {
            throw new Error(`API Error: ${response.status} ${response.statusText}`);
        }

        // ðŸŒŸ CRITICAL DEBUGGING LINE ADDED HERE ðŸŒŸ
        const result = await response.json(); 
        console.log("ðŸ”¥ Raw Gemini Response for Debugging:", result); 
        // ðŸ‘† Look at this log when the error occurs!

        const botText = result.candidates?.[0]?.content?.parts?.[0]?.text;

        if (botText) {
            chatHistory.push({ role: "model", parts: [{ text: botText }] });
            // Use browser's built-in TTS as fallback since Gemini TTS API structure is complex
            speakText(botText);
        } else {
            // This is the line that throws the error you see:
            throw new Error("No response text from Gemini API");
        }
    } catch (error) {
        showError(`Error getting response: ${error.message}`);
    }
}

        /**
         * Speaks text using the browser's built-in SpeechSynthesis API
         * @param {string} textToSpeak - The text to be spoken.
         */
        function speakText(textToSpeak) {
            // Use browser's built-in TTS as a reliable fallback
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(textToSpeak);
                utterance.rate = 0.9;
                utterance.pitch = 1;
                utterance.volume = 1;
                
                utterance.onstart = () => updateUI('speaking');
                utterance.onend = () => updateUI('idle');
                utterance.onerror = () => {
                    showError('Speech synthesis failed');
                    updateUI('idle');
                };
                
                speechSynthesis.speak(utterance);
            } else {
                showError('Speech synthesis not supported in this browser');
                updateUI('idle');
            }
        }
    </script>
</body>
</html>